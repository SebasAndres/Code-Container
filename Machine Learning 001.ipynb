{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def getReflexion ():\n",
    "    data = input (\"> Hola :), : \")\n",
    "    time.sleep(2);\n",
    "    data\n",
    "    print (sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer ():\n",
    "    def __init__ (self, f_act, n_conn, n_neur):\n",
    "        self.f_act = f_act\n",
    "        self.n_neur = n_neur\n",
    "\n",
    "        self.B = np.random.rand(1, n_neur)      * 2 - 1\n",
    "        self.W = np.random.rand(n_conn, n_neur) * 2 - 1\n",
    "            \n",
    "class neuralNetwork ():        \n",
    "    def __init__ (self, topology, f_act, nInputs, l_cost, dDs, l_rate=0.05):\n",
    "        self.topology = topology        \n",
    "        self.f_act = f_act \n",
    "        self.l_rate = l_rate\n",
    "        self.l_cost = l_cost\n",
    "        self.dDs = dDs\n",
    "                \n",
    "        self.net = [Layer(f_act, nInputs, topology[0])]\n",
    "        for l in range(len(topology)):\n",
    "            self.net.append (Layer(f_act, topology[l-1], topology[l]))\n",
    "                \n",
    "    def fPropagate (self, Input):\n",
    "        Out = [(None, Input)]\n",
    "        for layer in self.net:           \n",
    "            Z = Out[-1][1] @ layer.W + layer.B\n",
    "            A = layer.f_act (Z);\n",
    "            Out.append ((Z, A))\n",
    "        return Out\n",
    "    \n",
    "    def bPropagate (self, Expected, Out):        \n",
    "        # delta = dC / dA * dA / dZ * dZ / dW\n",
    "        #            dCA   x   dAZ  x   dZW                     \n",
    "        for L, layer in enumerate(reversed(self.net)):\n",
    "            deltaW = dDs[0](Expected, Out[L][1]) * dDs[1](Out[L][0]) * Out[L-1][1]\n",
    "            deltaB = dDs[0](Expected, Out[L][1]) * dDs[1](Out[L][0])            \n",
    "            layer.W -= self.l_rate * deltaW\n",
    "            layer.B -= self.l_rate * deltaB\n",
    "            \n",
    "    def showShapes (self):\n",
    "        for j, layer in enumerate(self.net):\n",
    "            print (\"# \",j); print (\"W\",layer.W.shape, \": \", layer.W); print (\"B\",layer.B.shape, \": \", layer.B, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funci칩n de activaci칩n\n",
    "sigm = lambda x: 1 / (1 + np.e ** (-x))\n",
    "f_error = lambda y, p: 0.5 * (y - p)**2\n",
    "\n",
    "# Dds = [dCA, dAZ, dZW]\n",
    "Dds = [lambda y, a: -y+a, lambda z: np.e**(z) / ((1 + np.e**(z))**2)]\n",
    "\n",
    "nNet = neuralNetwork([2,4,6,8,12, 2], sigm, 2, f_error, Dds)\n",
    "\n",
    "nNet.fPropagate(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hola :), escribe un pedido: ?\n",
      "\n",
      " El tiempo es un bloque con lo que somos, fuimos y seremos. Es sencillo de entender pero dificil de explicar. Una coordenada m치s para ubicarnos, pero al fin, una ilusi칩n.\n"
     ]
    }
   ],
   "source": [
    "getReflexion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
